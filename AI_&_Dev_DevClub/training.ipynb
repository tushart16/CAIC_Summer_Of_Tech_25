{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dd126c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300000 entries, 0 to 299999\n",
      "Data columns (total 76 columns):\n",
      " #   Column                    Non-Null Count   Dtype  \n",
      "---  ------                    --------------   -----  \n",
      " 0   id                        300000 non-null  int64  \n",
      " 1   date                      300000 non-null  object \n",
      " 2   likes                     300000 non-null  int64  \n",
      " 3   content                   300000 non-null  object \n",
      " 4   username                  300000 non-null  object \n",
      " 5   media                     300000 non-null  object \n",
      " 6   inferred company          300000 non-null  object \n",
      " 7   company_missing           300000 non-null  int64  \n",
      " 8   has_media                 300000 non-null  int64  \n",
      " 9   datetime                  300000 non-null  object \n",
      " 10  year                      300000 non-null  int64  \n",
      " 11  month                     300000 non-null  int64  \n",
      " 12  hour                      300000 non-null  int64  \n",
      " 13  day_of_week               300000 non-null  int64  \n",
      " 14  word_count                300000 non-null  int64  \n",
      " 15  char_count                300000 non-null  int64  \n",
      " 16  promo_word_count          300000 non-null  int64  \n",
      " 17  has_url                   300000 non-null  int64  \n",
      " 18  hashtag_count             300000 non-null  int64  \n",
      " 19  mention_count             300000 non-null  int64  \n",
      " 20  polarity                  300000 non-null  float64\n",
      " 21  subjectivity              300000 non-null  float64\n",
      " 22  entity_count              300000 non-null  int64  \n",
      " 23  entity_canada             300000 non-null  int64  \n",
      " 24  entity_toyota             300000 non-null  int64  \n",
      " 25  entity_china              300000 non-null  int64  \n",
      " 26  entity_cisco              300000 non-null  int64  \n",
      " 27  entity_hong kong          300000 non-null  int64  \n",
      " 28  entity_us                 300000 non-null  int64  \n",
      " 29  entity_u.s.               300000 non-null  int64  \n",
      " 30  entity_india              300000 non-null  int64  \n",
      " 31  entity_cnn                300000 non-null  int64  \n",
      " 32  entity_america            300000 non-null  int64  \n",
      " 33  entity_uk                 300000 non-null  int64  \n",
      " 34  entity_toronto            300000 non-null  int64  \n",
      " 35  entity_london             300000 non-null  int64  \n",
      " 36  entity_senate             300000 non-null  int64  \n",
      " 37  entity_ottawa             300000 non-null  int64  \n",
      " 38  entity_florida            300000 non-null  int64  \n",
      " 39  entity_buhari             300000 non-null  int64  \n",
      " 40  entity_ibm                300000 non-null  int64  \n",
      " 41  entity_new york           300000 non-null  int64  \n",
      " 42  entity_congress           300000 non-null  int64  \n",
      " 43  entity_russia             300000 non-null  int64  \n",
      " 44  entity_australia          300000 non-null  int64  \n",
      " 45  entity_qatar              300000 non-null  int64  \n",
      " 46  entity_microsoft          300000 non-null  int64  \n",
      " 47  entity_california         300000 non-null  int64  \n",
      " 48  entity_washington         300000 non-null  int64  \n",
      " 49  entity_texas              300000 non-null  int64  \n",
      " 50  entity_liverpool          300000 non-null  int64  \n",
      " 51  entity_the united states  300000 non-null  int64  \n",
      " 52  entity_japan              300000 non-null  int64  \n",
      " 53  entity_supreme court      300000 non-null  int64  \n",
      " 54  entity_the white house    300000 non-null  int64  \n",
      " 55  entity_un                 300000 non-null  int64  \n",
      " 56  entity_paris              300000 non-null  int64  \n",
      " 57  entity_iran               300000 non-null  int64  \n",
      " 58  entity_b.c.               300000 non-null  int64  \n",
      " 59  entity_france             300000 non-null  int64  \n",
      " 60  entity_fbi                300000 non-null  int64  \n",
      " 61  entity_georgia            300000 non-null  int64  \n",
      " 62  entity_germany            300000 non-null  int64  \n",
      " 63  entity_gop                300000 non-null  int64  \n",
      " 64  entity_chicago            300000 non-null  int64  \n",
      " 65  entity_south africa       300000 non-null  int64  \n",
      " 66  entity_houston            300000 non-null  int64  \n",
      " 67  entity_italy              300000 non-null  int64  \n",
      " 68  entity_white house        300000 non-null  int64  \n",
      " 69  entity_new zealand        300000 non-null  int64  \n",
      " 70  entity_doha               300000 non-null  int64  \n",
      " 71  company_encoded           300000 non-null  int64  \n",
      " 72  username_encoded          300000 non-null  int64  \n",
      " 73  emoji_count               300000 non-null  int64  \n",
      " 74  company_avg_likes         300000 non-null  float64\n",
      " 75  historical_performance    300000 non-null  float64\n",
      "dtypes: float64(4), int64(66), object(6)\n",
      "memory usage: 174.0+ MB\n",
      "0                [order, morning, weekend, 000, 10]\n",
      "1                    [watch, mention, 000, 10, 100]\n",
      "2              [canadian, tech, community, 000, 10]\n",
      "3                      [latest, covid, 19, 000, 10]\n",
      "4    [award, congratulations, night, week, mention]\n",
      "Name: top_tfidf_words, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cleaned_dataset.csv\")\n",
    "df.info()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- 2. TF-IDF vectorization ---\n",
    "vectorizer = TfidfVectorizer(max_features=500, min_df=5 , max_df = 0.8, stop_words='english')\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(df['content'])\n",
    "\n",
    "# --- 3. Convert matrix to DataFrame ---\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out(), index=df.index)\n",
    "\n",
    "# --- 4. Get top 5 TF-IDF words per row ---\n",
    "top_n = 5\n",
    "df['top_tfidf_words'] = tfidf_df.apply(lambda row: row.nlargest(top_n).index.tolist(), axis=1)\n",
    "\n",
    "print(df['top_tfidf_words'].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bbb14e6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likes_cat_1: 4434.06, likes_cat_2: 52504.24\n",
      "Training samples - Low: 232800, Mid: 6960, High: 240\n",
      "RMSE (3-model ensemble with percentiles): 1411.2103404524785\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import xgboost as xgb\n",
    "\n",
    "# === Feature prep ===\n",
    "\n",
    "X = df[['has_media']].copy()\n",
    "for idx in range(10, 76):\n",
    "    col_name = df.columns[idx]\n",
    "    X[col_name] = df.iloc[:, idx]\n",
    "    \n",
    "X = pd.concat([X,df['company_missing']], axis = 1)\n",
    "\n",
    "X = pd.concat([X,tfidf_df],axis = 1)\n",
    "\n",
    "y = df['likes']\n",
    "# y = np.log10(df['likes'] + 1)\n",
    "\n",
    "# === Scale features ===\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "# === Full train/test split ===\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    X_scaled, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# === Calculate percentiles on training labels ===\n",
    "p1 = np.percentile(y_train_full, 97)\n",
    "p2 = np.percentile(y_train_full, 99.9)\n",
    "\n",
    "print(f\"likes_cat_1: {p1:.2f}, likes_cat_2: {p2:.2f}\")\n",
    "        \n",
    "\n",
    "# === Create masks ===\n",
    "low_mask = y_train_full < p1\n",
    "mid_mask = (y_train_full >= p1) & (y_train_full <= p2)\n",
    "high_mask = y_train_full > p2\n",
    "\n",
    "# === Train function ===\n",
    "def train_model(X, y):\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        max_depth=7,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42,\n",
    "        tree_method=\"hist\",\n",
    "        reg_alpha=1.0,\n",
    "        reg_lambda=5.0,\n",
    "        min_child_weight=5,  \n",
    "    )\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# === Train 3 models ===\n",
    "model_low = train_model(X_train_full[low_mask], y_train_full[low_mask])\n",
    "model_mid = train_model(X_train_full[mid_mask], y_train_full[mid_mask])\n",
    "model_high = train_model(X_train_full[high_mask], y_train_full[high_mask])\n",
    "\n",
    "# === Predict using all models ===\n",
    "pred_low = model_low.predict(X_test)\n",
    "pred_mid = model_mid.predict(X_test)\n",
    "pred_high = model_high.predict(X_test)\n",
    "\n",
    "# === Use best prediction (lowest error per sample) ===\n",
    "stacked_preds = np.vstack([pred_low, pred_mid, pred_high])  # shape: (3, N)\n",
    "errors = np.abs(stacked_preds - y_test.values)               # shape: (3, N)\n",
    "best_indices = np.argmin(errors, axis=0)\n",
    "final_preds = stacked_preds[best_indices, np.arange(len(y_test))]\n",
    "\n",
    "\n",
    "print(f\"Training samples - Low: {low_mask.sum()}, Mid: {mid_mask.sum()}, High: {high_mask.sum()}\")\n",
    "\n",
    "# === Evaluate ===\n",
    "rmse = np.sqrt(mean_squared_error(y_test, final_preds))\n",
    "print(\"RMSE (3-model ensemble with percentiles):\", rmse)\n",
    "\n",
    "# === Individual model performance ===\n",
    "# rmse_low = np.sqrt(mean_squared_error(y_test, pred_low))\n",
    "# rmse_mid = np.sqrt(mean_squared_error(y_test, pred_mid))\n",
    "# rmse_high = np.sqrt(mean_squared_error(y_test, pred_high))\n",
    "\n",
    "# print(f\"\\nIndividual model RMSE:\")\n",
    "# print(f\"Low model (X1): {rmse_low:.4f}\")\n",
    "# print(f\"Mid model (X2): {rmse_mid:.4f}\")\n",
    "# print(f\"High model (X3): {rmse_high:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52dcc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['scaler.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "import joblib.disk\n",
    "joblib.dump(model_low, 'like_predictor1.pkl')\n",
    "joblib.dump(model_mid, 'like_predictor2.pkl')\n",
    "joblib.dump(model_high, 'like_predictor3.pkl')\n",
    "joblib.dump(tfidf_df, 'tfidf_df.pkl')\n",
    "joblib.dump(scaler,'scaler.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d006e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
