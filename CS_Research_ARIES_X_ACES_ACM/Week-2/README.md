# **WEEK-2**

## **From Neurons to Networks**

## By: ARIES X ACES ACM

## **Focus Areas:**

* Neural Networks from scratch  
* Basics of PyTorch  
* CNNs: Motivation \+ Overview  
* Project: Fashion-MNIST Classification


# Part 1: Neural Networks

## **Objective:**

Understand how a neural network functions internally — forward pass, activation functions, backpropagation, and manual weight updates.

##  **Expectations:**

* Implement a neural network with one hidden layer using only NumPy  
* Include manual forward \+ backward passes  
* Train it on a small dataset  
* Bonus: Try with multiple activation functions (ReLU, Sigmoid)

## **Resources:**

* [Neural Networks- Youtube](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)   
  Watch the first and second videos for intuition.

| Build a visual and intuitive understanding of how a Neural Network is structured and how it learns from a training dataset. |
| :---- |


* [Machine Learning for Beginners: An Introduction to Neural Networks \-](https://victorzhou.com/blog/intro-to-neural-networks/) [victorzhou.com](http://victorzhou.com)

| This article walks you through neural networks with a code-first mindset. Try to re-implement each block by yourself to get real hands-on understanding. |
| :---- |

* [Choosing a loss function \- perplexity](https://www.perplexity.ai/page/choosing-a-loss-function-c8Vpl.DIS6WusUpDJSIcRQ)

| Clears up smaller but crucial concepts around loss functions and gradient descent that often get skipped — helpful for tying everything together after doing the main tutoria |
| :---- |

# Part 2: PyTorch- A Smarter Way

## **Objective:**

Learn to use PyTorch to define and train neural networks with fewer lines of code, better performance, and real-world scalability.

## **Expectations:**

* Understand how PyTorch works under the hood and how it relates to NumPy — especially in terms of tensors, broadcasting, and gradients.  
    
* Get hands-on with model definition, forward pass, loss calculation, and optimization in PyTorch. By the end, you should be able to build your first working Neural Network using the PyTorch framework.

## **Resources:**

*   [Pytorch Tutorial for Deep Learning Lovers](https://www.kaggle.com/code/kanncaa1/pytorch-tutorial-for-deep-learning-lovers)

| Till “Variables” topic – To understand how PyTorch relates to NumPy and how tensor operations work under the hood. |
| :---- |

* [Your First Neural Network in PyTorch](https://www.youtube.com/watch?v=OIenNRt2bjg)

| Starts from the basics and builds up to implementing a neural network — perfect for getting comfortable with PyTorch by doing. |
| :---- |


  

# Part 3: Intro to CNN: What’s the big deal?

## **Objective:** 

Get conceptual clarity on **why** CNNs are used for images, how convolutions work, and what layers like pooling, padding, and filters mean.

## **Expectations:**

* Understand the architectural difference between Dense NNs and CNNs  
* Know what convolutional layers do (without coding them yet)  
* Be able to explain when/why to use a CNN

## **Resources:**

* [CNNs Explained](https://youtu.be/oGpzWAlP5p0?si=lQlogPtUsNMWc1T_) 

| A visual and intuitive explanation of CNNs — great for understanding how neural nets can handle images using convolutional layers. |
| :---- |

# Submission Details:

Use the Fashion-MNIST dataset (links below) to build and train your neural network model. Submit:

* Your model code  
* A report showing the classification accuracy on the test set

Fashion-MNIST is a tougher, direct replacement for MNIST — which you’ve seen before. This is mostly for practice and getting comfortable with real datasets.

**Useful links:**  
 GitHub: [https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)  
 Kaggle: [https://www.kaggle.com/datasets/zalando-research/fashionmnist](https://www.kaggle.com/datasets/zalando-research/fashionmnist)  
 Benchmark: [http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/](http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/)

Make sure your code is clean and well-commented. Submit via the Google Form (link shared in the group) and confirm you’ve joined the WhatsApp group.

# Extra Resources \- For the curious

#### (NOT COMPULSORY\!\!)

* #### [Data Normalization Explained: Types, Examples, & Methods | Estuary](https://estuary.dev/blog/data-normalization/)

| Explains what data normalization is, why it matters, and how to apply different types (like min-max or z-score) in ML. |
| :---- |

* #### [ReLU vs. Sigmoid Function in Deep Neural Networks | Weights & Biases](https://wandb.ai/ayush-thakur/dl-question-bank/reports/ReLU-vs-Sigmoid-Function-in-Deep-Neural-Networks--VmlldzoyMDk0MzI)

| A comparison of activation functions (ReLU vs. Sigmoid), their use-cases, and where each one wins or fails. |
| :---- |

* #### [A Gentle Introduction To Sigmoid Function – Machine Learning Mastery](https://machinelearningmastery.com/a-gentle-introduction-to-sigmoid-function/)

| Breaks down the sigmoid function step by step with visuals — great for understanding how it behaves and why it’s used. |
| :---- |

* #### [Loss Functions in Machine Learning Explained | DataCamp](https://www.datacamp.com/tutorial/loss-function-in-machine-learning)

| Overview of common loss functions and when to use each , makes it easier to connect math with model behavior. |
| :---- |

* #### [The Math Behind Stochastic Gradient Descent | Towards Data Science](https://spotintelligence.com/2024/02/19/learning-rate-machine-learning)

| A solid mathematical walkthrough of SGD — useful if you want to understand how updates really happen under the hood. |
| :---- |

* #### [Learning Rate in Machine Learning | Spotintelligence](https://spotintelligence.com/2024/02/19/learning-rate-machine-learning)

| Simple but deep dive into learning rate — how it affects model performance, training time, and convergence. |
| :---- |

* #### [Initializing Neural Networks – DeepLearning.AI](https://www.deeplearning.ai/ai-notes/initialization/index.html)

| Talks about why proper weight initialization is critical and how bad starts can kill your model before it even learns. |
| :---- |

* #### [Neural Networks from Scratch — The Full Ride](https://youtube.com/playlist?list=PLQVvvaa0QuDcjD5BAw2DxE6OF2tius3V3&si=AgNzTkm2FK9uFAm5)

| A full playlist that takes you from absolute basics to building a neural net from scratch — great if you wanna go deep at your own pace. |
| :---- |

####  

# Content Contributors:

* Purushottam Sharma  
* Anmol Goel  
* Jahnabi Roy  
* Pallav  
* Tamanna  
* Reyansh  
* Nideesh  
* Avaneesh  
* Ishant