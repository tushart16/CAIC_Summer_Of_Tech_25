# ğŸš€ CAIC Summer of Technology 2025  
## ğŸ§  Machine Learning + Development Track: Week 1  
### ğŸ” Problem Understanding, Dataset Exploration & Dev Planning  

---

## ğŸ—’ Problem Statement

We're building a system to help marketers understand and optimize their tweets using machine learning. Two core tasks:

1. **Predict tweet likes** from metadata (text, timestamp, media)
2. **Generate tweet text** from metadata (brand, media, etc.)

Youâ€™ll work with ~300K tweets and eventually integrate your models into a user-facing app.

---

## ğŸ¯ Week 1 Objectives

Before we model anything, we need to:
- Understand the dataset
- Clean and analyze it
- Plan how it will be used in an API
- Identify the features weâ€™ll engineer for models
- Think ahead about frontend/backend integration

---

## ğŸ§° 1. Environment Setup

Use **Google Colab** this week.

Pros:
- No setup required
- Integrated with Google Drive
- Great for collaboration

Optional: Setup Jupyter locally with `pandas`, `matplotlib`, `seaborn`, `textblob`.

---

## ğŸ“¦ 2. Load the Dataset

```python
import pandas as pd

df = pd.read_csv("path_to_data.csv")  # replace with your actual path

df.shape
df.head()
df.info()
df.isnull().sum()
```

---

## ğŸ§¼ 3. Light Preprocessing (for EDA and Dev Readiness)

```python
df.dropna(subset=['content', 'username', 'company', 'likes'], inplace=True)
df['media'].fillna('no_media', inplace=True)
df['has_media'] = df['media'].apply(lambda x: x != 'no_media')
df['content'] = df['content'].astype(str).str.strip().str.lower()
df['datetime'] = pd.to_datetime(df['date'], errors='coerce')
```

This gives us:
- Clean features for EDA
- Fields we can send via API (in future)

---

## ğŸ“Š 4. Exploratory Data Analysis

```python
import seaborn as sns
import matplotlib.pyplot as plt

sns.histplot(df['likes'], kde=True)
sns.boxplot(x=df['likes'])
```

```python
df['hour'] = df['datetime'].dt.hour
df['day_of_week'] = df['datetime'].dt.day_name()
```

```python
df['word_count'] = df['content'].apply(lambda x: len(x.split()))
df['char_count'] = df['content'].apply(len)
```

---

### ğŸ§  What EDA Should Help You Plan

Think like a developer:
- What features can be passed to an ML model in an API request?
- Which fields should the frontend ask users for?
- What preprocessing should happen inside the API?
- Can we calculate features (e.g., word count, has_media) *inside* the backend?

---

## ğŸ› ï¸ 5. Feature Plan for APIs

These are likely features to use:
- `company`, `username`
- `content` â†’ `word_count`, `sentiment`
- `media` â†’ `has_media`
- `datetime` â†’ hour, weekday

Make a note of this mapping! Youâ€™ll use it in the backend.

---

## ğŸ§  Bonus Ideas

| Topic | Use |
|--|--|
| WordClouds | Visualize engaging keywords |
| TextBlob | Get sentiment polarity |
| TF-IDF | Later for text vectorization |
| t-SNE | Explore clustering of content |
| Named Entity Recognition | See what brands/places are mentioned |

---

## ğŸ“š Resources

- [EDA Guide â€“ Kaggle](https://www.kaggle.com/learn/data-cleaning)  
- [Feature Engineering â€“ Kaggle](https://www.kaggle.com/learn/feature-engineering)  
- [Streamlit Docs](https://docs.streamlit.io/)  
- [FastAPI Tutorial](https://fastapi.tiangolo.com/tutorial/)  

---

## âœ… Week 1 Deliverables

- âœ… Cleaned dataset
- âœ… EDA notebook with plots & insights
- âœ… Feature list planned for modeling
- âœ… Plan for API input/output structure

---

## ğŸ”® Whatâ€™s Next (Week 2)

- Build and train a model to predict likes (Task 1)
- Wrap it in an API using Flask or FastAPI
- Accept metadata and return predicted likes

---  
Build smart. Build useful. Letâ€™s get started ğŸš€
